{"nbformat": 4, "nbformat_minor": 5, "metadata": {}, "cells": [{"id": "aa242cfa", "cell_type": "markdown", "source": "## Step 1: Load and Inspect the Data  \n\n**What we expect:** We will load the dataset containing TikTok video performance metrics to understand its structure and review sample records. We expect to see columns such as Views, Likes, Comments, Shares, Duration, Hashtags_Count and follower counts (if available).  \n\n**Instructions:** Load the CSV file `tiktok_video_performance_v2.csv` into a pandas DataFrame using `pandas.read_csv()`. Display the first few rows and review the shape of the data to understand the number of samples and features available."}, {"id": "e708b91c", "cell_type": "code", "source": "# Load the dataset\nimport pandas as pd\n\ndf = pd.read_csv('tiktok_video_performance_v2.csv')\nprint('Dataset shape:', df.shape)\ndf.head()", "metadata": {}, "outputs": []}, {"id": "93946850", "cell_type": "markdown", "source": "**What we learned from Step 1**  \n\nLoading the dataset gives us the initial shape and a preview of the data. We can see the number of rows (videos) and columns (features). This helps us plan feature engineering and model training in later steps."}, {"id": "38fc7db5", "cell_type": "markdown", "source": "## Step 2: Engineer Features and Target  \n\n**What we expect:** We will create a target variable **popular** based on whether a video’s like count is above the median. We also select relevant numerical features that might predict popularity, such as Views, Comments, Shares, and Follower count (if available).  \n\n**Instructions:** Compute the median of the `Likes` column to define a binary target. Then select a subset of feature columns that will be used to train our model. Display basic statistics of the features to understand their distribution."}, {"id": "b0f6408e", "cell_type": "code", "source": "# Engineer target and select features\nimport numpy as np\n\n# Create target: 1 if Likes >= median, else 0\nmedian_likes = df['Likes'].median()\ndf['popular'] = (df['Likes'] >= median_likes).astype(int)\n\n# Select relevant numerical features\nfeature_cols = ['Views', 'Comments', 'Shares']\nif 'Followers' in df.columns:\n    feature_cols.append('Followers')\n\nX = df[feature_cols]\ny = df['popular']\n\n# Show basic statistics of features\nX.describe()", "metadata": {}, "outputs": []}, {"id": "a546a20e", "cell_type": "markdown", "source": "**What we learned from Step 2**  \n\nBy creating the **popular** target based on the median of likes and selecting numerical features, we prepared the dataset for model training. The summary statistics provide insights into the scale and distribution of each feature, which may help when tuning models."}, {"id": "592d9def", "cell_type": "markdown", "source": "## Step 3: Train a Random Forest Classifier  \n\n**What we expect:** We'll split the data into training and test sets, then train a Random Forest classifier to predict video popularity. We expect the model to capture relationships between features (views, comments, shares, followers) and the popularity target.  \n\n**Instructions:** Use `train_test_split` to create training and test sets. Instantiate a `RandomForestClassifier`, fit it on the training data, and generate predictions for the test set."}, {"id": "6e28dc0a", "cell_type": "code", "source": "# Train a Random Forest classifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train model\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\n\n# Predictions\ny_pred = rf.predict(X_test)\ny_pred[:10]", "metadata": {}, "outputs": []}, {"id": "b7c1e23e", "cell_type": "markdown", "source": "**What we learned from Step 3**  \n\nTraining the Random Forest classifier allows us to capture nonlinear relationships between the features and the target. The model outputs a prediction for whether a video is likely to be popular based on the features. At this stage, we have a trained model and some initial predictions."}, {"id": "d1d6116f", "cell_type": "markdown", "source": "## Step 4: Evaluate the Model and Interpret Results  \n\n**What we expect:** We will assess the model’s performance on the test set using metrics such as accuracy, precision, recall, and F1-score. We will also visualize the confusion matrix and examine feature importances to understand which variables influence the model most.  \n\n**Instructions:** Use `classification_report` and `confusion_matrix` from scikit-learn to evaluate the predictions. Plot the confusion matrix using `matplotlib` and compute feature importances from the Random Forest. Present the results in a readable format."}, {"id": "cbe0d371", "cell_type": "code", "source": "# Evaluate model performance and interpret results\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# Classification report\nreport = classification_report(y_test, y_pred, output_dict=True)\nreport_df = pd.DataFrame(report).transpose()\nprint('Classification Report:')\ndisplay(report_df)\n\n# Confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nplt.figure()\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\n\n# Feature importances\nimportances = rf.feature_importances_\nimportance_df = pd.DataFrame({'feature': feature_cols, 'importance': importances}).sort_values(by='importance', ascending=False)\nprint('Feature Importances:')\ndisplay(importance_df)", "metadata": {}, "outputs": []}, {"id": "99d03bb7", "cell_type": "markdown", "source": "**What we learned from Step 4**  \n\nThe classification report and confusion matrix provide insights into the model's accuracy and how well it distinguishes between popular and non-popular videos. The feature importances highlight which metrics (e.g., views, comments, shares, followers) most influence the model’s predictions. This helps us understand and explain the model's decisions."}, {"id": "056eeb44", "cell_type": "markdown", "source": "## Step 5: Fairness Check  \n\n**What we expect:** We will check whether the model exhibits bias across different groups. If the dataset includes a column like `author_verified`, we can compare the positive prediction rates for verified vs. non-verified creators.  \n\n**Instructions:** If the `author_verified` column exists, calculate the proportion of videos predicted as popular for both verified and non-verified creators. Report the difference and discuss potential bias."}, {"id": "bfa6220f", "cell_type": "code", "source": "# Fairness check\nif 'author_verified' in df.columns:\n    df_test = X_test.copy()\n    df_test['author_verified'] = df.loc[X_test.index, 'author_verified']\n    df_test['prediction'] = y_pred\n    # positive rate by verification status\n    rates = df_test.groupby('author_verified')['prediction'].mean()\n    print('Positive prediction rates by author verification status:')\n    print(rates)\n    if len(rates) == 2:\n        bias = abs(rates.iloc[0] - rates.iloc[1])\n        print('Difference in positive rates:', bias)\nelse:\n    print('No author_verified column found; skipping fairness check.')", "metadata": {}, "outputs": []}, {"id": "589c7644", "cell_type": "markdown", "source": "**What we learned from Step 5**  \n\nBy comparing positive prediction rates across groups (e.g., verified vs. non-verified creators), we can detect potential bias in the model. If there is a significant difference between groups, it might indicate that the model is unfairly favoring one group. Otherwise, the model appears to treat creators similarly regardless of verification status."}, {"id": "8b2784ec", "cell_type": "markdown", "source": "## Conclusion  \n\nIn this notebook, we built a pipeline to predict whether a TikTok video will be popular using a Random Forest classifier. We followed a step-by-step approach inspired by the Spotify workshop examples, including expectations and reflections for each step. We loaded and explored the dataset, engineered features and a target, trained a model, evaluated it, and performed a fairness check. The model's performance and feature importances provide insights into what drives video popularity, and the fairness check ensures that the model's predictions are equitable across different groups."}]}
