{"nbformat": 4, "nbformat_minor": 5, "metadata": {}, "cells": [{"id": "0ae1b6fb", "cell_type": "markdown", "source": "## Step 1: Load and Inspect the Data  \n\n**What we expect:** We will load the dataset containing TikTok video performance metrics to understand its structure and review sample records. We expect to see columns such as play counts, comment counts, share counts, like counts and follower counts.  \n\n**Instructions:** Load the CSV file `tiktok_video_performance_v2.csv` into a pandas DataFrame. Display the first few rows and review the shape of the data to understand the number of samples and features available.\n", "metadata": {}}, {"id": "d0e047fd", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# Load the dataset\nimport pandas as pd\n\ndf = pd.read_csv('tiktok_video_performance_v2.csv')\nprint('Dataset shape:', df.shape)\ndf.head()", "outputs": []}, {"id": "f0b63d45", "cell_type": "markdown", "source": "### What we learned from Step 1  \n\nLoading the dataset gives us the initial shape and a preview of the data. We can see the number of rows (videos) and columns (features). This helps us plan feature engineering and model training in later steps.", "metadata": {}}, {"id": "dd52b973", "cell_type": "markdown", "source": "## Step 2: Engineer Features and Target  \n\n**What we expect:** We will create a target variable `popular` based on whether a video\u2019s like count is above the median. We also select relevant numerical features that might predict popularity, such as play count, comment count, share count, and follower count.  \n\n**Instructions:** Compute the median of the `like_count` column to define a binary target. Then select a subset of feature columns that will be used to train our model. Display basic statistics of the features to understand their distribution.\n", "metadata": {}}, {"id": "61a0b2f5", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# Engineer target and select features\nimport numpy as np\n\n# Create target: 1 if like_count >= median, else 0\nmedian_likes = df['like_count'].median()\ndf['popular'] = (df['like_count'] >= median_likes).astype(int)\n\n# Select relevant numerical features\nfeature_cols = ['play_count', 'comment_count', 'share_count', 'author_follower_count']\nX = df[feature_cols]\ny = df['popular']\n\n# Show basic statistics of features\nX.describe()", "outputs": []}, {"id": "a16fbe0b", "cell_type": "markdown", "source": "### What we learned from Step 2  \n\nBy creating the `popular` target and selecting numerical features, we prepared the dataset for model training. The summary statistics provide insights into the scale and distribution of each feature, which may help when tuning models.", "metadata": {}}, {"id": "f85654b4", "cell_type": "markdown", "source": "## Step 3: Train a Random Forest Classifier  \n\n**What we expect:** Using a tree-based ensemble model like Random Forest should capture non-linear relationships between features and the target. We expect the model to perform reasonably well on the classification task.  \n\n**Instructions:** Split the data into training and test sets. Train a `RandomForestClassifier` on the training data. After training, output the first few predictions on the test set to see how the model classifies examples.\n", "metadata": {}}, {"id": "05be5b27", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# Train a Random Forest classifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and train the model\nrf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_clf.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = rf_clf.predict(X_test)\ny_pred[:10]", "outputs": []}, {"id": "9483b742", "cell_type": "markdown", "source": "### What we learned from Step 3  \n\nTraining the Random Forest classifier provides us with a predictive model capable of classifying videos as popular or not based on the selected features. The first few predictions give us a sense of the model\u2019s output and confirm that the training process completed successfully.", "metadata": {}}, {"id": "5891f9e8", "cell_type": "markdown", "source": "## Step 4: Evaluate the Model and Interpret Results  \n\n**What we expect:** A classification report and confusion matrix will show how well the model distinguishes popular videos. Feature importances will indicate which metrics are most influential in predicting popularity.  \n\n**Instructions:** Generate a classification report and confusion matrix for the test set. Also extract feature importances from the trained model and display them in a table. Interpret these metrics to understand the model\u2019s strengths and weaknesses.\n", "metadata": {}}, {"id": "aa538efc", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# Evaluate the model and interpret results\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\n\n# Classification report\nreport = classification_report(y_test, y_pred, output_dict=False)\nprint(report)\n\n# Confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nprint('Confusion matrix:')\nprint(cm)\n\n# Feature importances\nimportances = rf_clf.feature_importances_\nfeature_importances = pd.DataFrame({'feature': feature_cols, 'importance': importances})\nfeature_importances.sort_values(by='importance', ascending=False)\n", "outputs": []}, {"id": "af9d6af2", "cell_type": "markdown", "source": "### What we learned from Step 4  \n\nThe classification report and confusion matrix tell us how accurately the model predicts popular videos. Higher precision and recall values indicate better performance. The confusion matrix reveals the number of true positives, true negatives, false positives, and false negatives. Feature importances highlight which variables have the greatest effect on the model\u2019s decisions, helping us understand the drivers of popularity.", "metadata": {}}, {"id": "61698814", "cell_type": "markdown", "source": "## Step 5: Fairness Check  \n\n**What we expect:** We want to ensure the model does not unfairly favor or disadvantage creators based on verification status. The positive prediction rate for verified and non-verified creators should be similar if the model is fair.  \n\n**Instructions:** If the dataset includes an `author_verified` column, append predictions to the test set and compute the mean predicted popularity for each group. Compare these rates to identify any potential bias. If the column is missing, note that fairness cannot be evaluated.\n", "metadata": {}}, {"id": "4a0e4a59", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# Fairness check\nif 'author_verified' in df.columns:\n    # Append predictions to test set\n    X_test_with_group = X_test.copy()\n    X_test_with_group['author_verified'] = df.loc[X_test.index, 'author_verified']\n    X_test_with_group['pred'] = y_pred\n\n    # Compute positive rate for each group\n    rates = X_test_with_group.groupby('author_verified')['pred'].mean()\n    print(\"Positive prediction rate by group:\")\n    print(rates)\nelse:\n    print(\"The column 'author_verified' does not exist in the dataset, so we cannot perform a fairness check.\")", "outputs": []}, {"id": "722470b5", "cell_type": "markdown", "source": "### What we learned from Step 5  \n\nIf the `author_verified` column exists, the output will show the positive prediction rate for verified and non-verified creators. A large gap between these rates might indicate potential bias in the model\u2019s predictions. If the column is absent, the fairness check cannot be performed and we note that the dataset does not include this information.", "metadata": {}}, {"id": "ce800141", "cell_type": "markdown", "source": "## Conclusion  \n\nIn this notebook we built a pipeline to predict whether a TikTok video will be popular based on features such as play count, comment count, share count, and follower count. We followed a structured approach: loading and exploring the data, engineering features, training a Random Forest classifier, evaluating its performance, interpreting feature importance, and checking for fairness. This step-by-step process not only produced a working model but also emphasized transparency and fairness, aligning with best practices for responsible AI.", "metadata": {}}]}
